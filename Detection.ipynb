{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from pathlib import Path\n",
    "\n",
    "import tkinter as tk\n",
    "\n",
    "# import the necessary packages\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from imutils.video import VideoStream\n",
    "from imutils.video import FPS\n",
    "import time\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hit space bar to capture\n",
    "def captureuser(name):\n",
    "    cam = cv2.VideoCapture(0)\n",
    "\n",
    "    cv2.namedWindow(\"capture\")\n",
    "\n",
    "    img_counter = 0\n",
    "    \n",
    "    dirname = f'dataset/{name}'\n",
    "    os.mkdir(dirname)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cam.read()\n",
    "        cv2.imshow(\"capture\", frame)\n",
    "        \n",
    "        if img_counter == 5:\n",
    "            cv2.destroyWindow(\"capture\")\n",
    "            break\n",
    "        if not ret:\n",
    "            break\n",
    "        k = cv2.waitKey(1)\n",
    "\n",
    "        if k%256 == 27:\n",
    "            # ESC pressed\n",
    "            print(\"Escape hit, closing...\")\n",
    "            break\n",
    "        elif k%256 == 32:\n",
    "            # SPACE pressed\n",
    "            path = f'dataset/{name}'\n",
    "            img_name = \"{}.jpg\".format(img_counter)\n",
    "            cv2.imwrite(os.path.join(path , img_name), frame)\n",
    "            cv2.imwrite(img_name, frame)\n",
    "            print(\"{} written!\".format(img_name))\n",
    "            img_counter += 1\n",
    "\n",
    "    cam.release()\n",
    "\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captureuser('tundeAdewole')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(unique_id,name,bank, password, account_balance):\n",
    "    import csv\n",
    "    \n",
    "    with open(r'bank_details.csv','a', newline = '\\n') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([unique_id,name,bank, password, account_balance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_csv('10234', 'Adewole Tunde 1', 'Gtbank', '12345', '20000.00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings():\n",
    "    #summary:\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # construct the argument parser and parse the arguments\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"-i\", \"--dataset\", required=True,\n",
    "        help=\"path to input directory of faces + images\")\n",
    "    ap.add_argument(\"-e\", \"--embeddings\", required=True,\n",
    "        help=\"path to output serialized db of facial embeddings\")\n",
    "    ap.add_argument(\"-d\", \"--detector\", required=True,\n",
    "        help=\"path to OpenCV's deep learning face detector\")\n",
    "    ap.add_argument(\"-m\", \"--embedding-model\", required=True,\n",
    "        help=\"path to OpenCV's deep learning face embedding model\")\n",
    "    ap.add_argument(\"-c\", \"--confidence\", type=float, default=0.5,\n",
    "        help=\"minimum probability to filter weak detections\")\n",
    "    #args = vars(ap.parse_args())\n",
    "    \n",
    "    # load our serialized face detector from disk\n",
    "    print(\"[INFO] loading face detector...\")\n",
    "\n",
    "    detector = cv2.dnn.readNetFromCaffe('face_detection_model/deploy.prototxt', 'face_detection_model/res10_300x300_ssd_iter_140000.caffemodel')\n",
    "    # load our serialized face embedding model from disk\n",
    "    embedder = cv2.dnn.readNetFromTorch('openface_nn4.small2.v1.t7')\n",
    "\n",
    "    # grab the paths to the input images in our dataset\n",
    "    print(\"[INFO] quantifying faces...\")\n",
    "    imagePaths = list(paths.list_images('dataset'))\n",
    "    # initialize our lists of extracted facial embeddings and\n",
    "    # corresponding people names\n",
    "    knownEmbeddings = []\n",
    "    knownNames = []\n",
    "    # initialize the total number of faces processed\n",
    "    total = 0\n",
    "    # loop over the image paths\n",
    "    for (i, imagePath) in enumerate(imagePaths):\n",
    "        # extract the person name from the image path\n",
    "        print(\"[INFO] processing image {}/{}\".format(i + 1,\n",
    "            len(imagePaths)))\n",
    "        name = imagePath.split(os.path.sep)[-2]\n",
    "\n",
    "        # load the image, resize it to have a width of 600 pixels (while\n",
    "        # maintaining the aspect ratio), and then grab the image\n",
    "        # dimensions\n",
    "        image = cv2.imread(imagePath)\n",
    "        image = imutils.resize(image, width=600)\n",
    "        (h, w) = image.shape[:2]\n",
    "        # construct a blob from the image\n",
    "        imageBlob = cv2.dnn.blobFromImage(\n",
    "            cv2.resize(image, (300, 300)), 1.0, (300, 300),\n",
    "            (104.0, 177.0, 123.0), swapRB=False, crop=False)\n",
    "\n",
    "        # apply OpenCV's deep learning-based face detector to localize\n",
    "        # faces in the input image\n",
    "        detector.setInput(imageBlob)\n",
    "        detections = detector.forward()\n",
    "\n",
    "        # ensure at least one face was found\n",
    "        if len(detections) > 0:\n",
    "            # we're making the assumption that each image has only ONE\n",
    "            # face, so find the bounding box with the largest probability\n",
    "            i = np.argmax(detections[0, 0, :, 2])\n",
    "            confidence = detections[0, 0, i, 2]\n",
    "\n",
    "            # ensure that the detection with the largest probability also\n",
    "            # means our minimum probability test (thus helping filter out\n",
    "            # weak detections)\n",
    "            if confidence > 0.5:\n",
    "                # compute the (x, y)-coordinates of the bounding box for\n",
    "                # the face\n",
    "                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "                (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "                # extract the face ROI and grab the ROI dimensions\n",
    "                face = image[startY:endY, startX:endX]\n",
    "                (fH, fW) = face.shape[:2]\n",
    "\n",
    "                # ensure the face width and height are sufficiently large\n",
    "                if fW < 20 or fH < 20:\n",
    "                    continue\n",
    "\n",
    "                # construct a blob for the face ROI, then pass the blob\n",
    "                # through our face embedding model to obtain the 128-d\n",
    "                # quantification of the face\n",
    "                faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255,\n",
    "                    (96, 96), (0, 0, 0), swapRB=True, crop=False)\n",
    "                embedder.setInput(faceBlob)\n",
    "                vec = embedder.forward()\n",
    "    \n",
    "                # add the name of the person + corresponding face\n",
    "                # embedding to their respective lists\n",
    "                knownNames.append(name)\n",
    "                knownEmbeddings.append(vec.flatten())\n",
    "                total += 1\n",
    "    # dump the facial embeddings + names to disk\n",
    "    print(\"[INFO] serializing {} encodings...\".format(total))\n",
    "    data = {\"embeddings\": knownEmbeddings, \"names\": knownNames}\n",
    "    f = open('output/embeddings.pickle', \"wb\")\n",
    "    f.write(pickle.dumps(data))\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading face detector...\n",
      "[INFO] quantifying faces...\n",
      "[INFO] processing image 1/23\n",
      "[INFO] processing image 2/23\n",
      "[INFO] processing image 3/23\n",
      "[INFO] processing image 4/23\n",
      "[INFO] processing image 5/23\n",
      "[INFO] processing image 6/23\n",
      "[INFO] processing image 7/23\n",
      "[INFO] processing image 8/23\n",
      "[INFO] processing image 9/23\n",
      "[INFO] processing image 10/23\n",
      "[INFO] processing image 11/23\n",
      "[INFO] processing image 12/23\n",
      "[INFO] processing image 13/23\n",
      "[INFO] processing image 14/23\n",
      "[INFO] processing image 15/23\n",
      "[INFO] processing image 16/23\n",
      "[INFO] processing image 17/23\n",
      "[INFO] processing image 18/23\n",
      "[INFO] processing image 19/23\n",
      "[INFO] processing image 20/23\n",
      "[INFO] processing image 21/23\n",
      "[INFO] processing image 22/23\n",
      "[INFO] processing image 23/23\n",
      "[INFO] serializing 21 encodings...\n"
     ]
    }
   ],
   "source": [
    "get_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    #summary\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"[INFO] loading face embeddings...\")\n",
    "    data = pickle.loads(open('output/embeddings.pickle', \"rb\").read())\n",
    "    le = LabelEncoder()\n",
    "    labels = le.fit_transform(data[\"names\"])\n",
    "    # train the model used to accept the 128-d embeddings of the face and\n",
    "    # then produce the actual face recognition\n",
    "    print(\"[INFO] training model...\")\n",
    "    recognizer = SVC(C=1.0, kernel=\"linear\", probability=True)\n",
    "    recognizer.fit(data[\"embeddings\"], labels)\n",
    "    # write the actual face recognition model to disk\n",
    "    f = open('output/recognizer.pickle', \"wb\")\n",
    "    f.write(pickle.dumps(recognizer))\n",
    "    f.close()\n",
    "\n",
    "    # write the label encoder to disk\n",
    "    f = open('output/le.pickle', \"wb\")\n",
    "    f.write(pickle.dumps(le))\n",
    "    f.close()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading face embeddings...\n",
      "[INFO] training model...\n"
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_check():\n",
    "    detector = cv2.dnn.readNetFromCaffe('face_detection_model/deploy.prototxt', 'face_detection_model/res10_300x300_ssd_iter_140000.caffemodel')\n",
    "    #summary\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # load our serialized face embedding model from disk\n",
    "    print(\"[INFO] loading face recognizer...\")\n",
    "    embedder = cv2.dnn.readNetFromTorch('openface_nn4.small2.v1.t7')\n",
    "\n",
    "    # load the actual face recognition model along with the label encoder\n",
    "    recognizer = pickle.loads(open('output/recognizer.pickle', \"rb\").read())\n",
    "    le = pickle.loads(open('output/le.pickle', \"rb\").read())\n",
    "\n",
    "    # initialize the video stream, then allow the camera sensor to warm up\n",
    "    print(\"[INFO] starting video stream...\")\n",
    "    vs = VideoStream(src=0).start()\n",
    "    time.sleep(2.0)\n",
    "\n",
    "    #run check for only 15seconds and then stop\n",
    "    timeout = time.time() + 15\n",
    "    \n",
    "    # start the FPS throughput estimator\n",
    "    fps = FPS().start()\n",
    "\n",
    "        # loop over frames from the video file stream\n",
    "    while True:\n",
    "        #run check for only 15seconds and then stop\n",
    "        if time.time() > timeout:\n",
    "            cv2.destroyWindow(\"Frame\")\n",
    "            break\n",
    "            \n",
    "        # grab the frame from the threaded video stream\n",
    "        frame = vs.read()\n",
    "\n",
    "        # resize the frame to have a width of 600 pixels (while\n",
    "        # maintaining the aspect ratio), and then grab the image\n",
    "        # dimensions\n",
    "        frame = imutils.resize(frame, width=600)\n",
    "        (h, w) = frame.shape[:2]\n",
    "\n",
    "        # construct a blob from the image\n",
    "        imageBlob = cv2.dnn.blobFromImage(\n",
    "            cv2.resize(frame, (300, 300)), 1.0, (300, 300),\n",
    "            (104.0, 177.0, 123.0), swapRB=False, crop=False)\n",
    "\n",
    "        # apply OpenCV's deep learning-based face detector to localize\n",
    "        # faces in the input image\n",
    "        detector.setInput(imageBlob)\n",
    "        detections = detector.forward()\n",
    "\n",
    "        #TODO: if 2 faces are detected alert the user of a warning\n",
    "        # loop over the detections\n",
    "        for i in range(0, detections.shape[2]):\n",
    "            # extract the confidence (i.e., probability) associated with\n",
    "            # the prediction\n",
    "            confidence = detections[0, 0, i, 2]\n",
    "\n",
    "            # filter out weak detections\n",
    "            if confidence > 0.5:\n",
    "                # compute the (x, y)-coordinates of the bounding box for\n",
    "                # the face\n",
    "                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "                (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "                # extract the face ROI\n",
    "                face = frame[startY:endY, startX:endX]\n",
    "                (fH, fW) = face.shape[:2]\n",
    "\n",
    "                # ensure the face width and height are sufficiently large\n",
    "                if fW < 20 or fH < 20:\n",
    "                    continue\n",
    "\n",
    "                # construct a blob for the face ROI, then pass the blob\n",
    "                # through our face embedding model to obtain the 128-d\n",
    "                # quantification of the face\n",
    "                faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255,\n",
    "                    (96, 96), (0, 0, 0), swapRB=True, crop=False)\n",
    "                embedder.setInput(faceBlob)\n",
    "                vec = embedder.forward()\n",
    "\n",
    "                # perform classification to recognize the face\n",
    "                preds = recognizer.predict_proba(vec)[0]\n",
    "                j = np.argmax(preds)\n",
    "                proba = preds[j]\n",
    "                name = le.classes_[j]\n",
    "\n",
    "                # draw the bounding box of the face along with the\n",
    "                # associated probability\n",
    "                text = \"{}: {:.2f}%\".format(name, proba * 100)\n",
    "                y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "                cv2.rectangle(frame, (startX, startY), (endX, endY),\n",
    "                    (0, 0, 255), 2)\n",
    "                cv2.putText(frame, text, (startX, y),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "\n",
    "        # update the FPS counter\n",
    "        fps.update()\n",
    "\n",
    "        # show the output frame\n",
    "        cv2.imshow(\"Frame\", frame)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        # if the `q` key was pressed, break from the loop\n",
    "        if key == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    # stop the timer and display FPS information\n",
    "    fps.stop()\n",
    "    print(\"[INFO] elasped time: {:.2f}\".format(fps.elapsed()))\n",
    "    print(\"[INFO] approx. FPS: {:.2f}\".format(fps.fps()))\n",
    "\n",
    "    # do a bit of cleanup\n",
    "    cv2.destroyAllWindows()\n",
    "    vs.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our serialized face detector from disk\n",
    "def recognize(imagePath):\n",
    "    #summary\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"[INFO] loading face detector...\")\n",
    "    detector = cv2.dnn.readNetFromCaffe('face_detection_model/deploy.prototxt', 'face_detection_model/res10_300x300_ssd_iter_140000.caffemodel')\n",
    "    \n",
    " \n",
    "    # load our serialized face embedding model from disk\n",
    "    print(\"[INFO] loading face recognizer...\")\n",
    "    embedder = cv2.dnn.readNetFromTorch('openface_nn4.small2.v1.t7')\n",
    " \n",
    "    # load the actual face recognition model along with the label encoder\n",
    "    recognizer = pickle.loads(open('output/recognizer.pickle', \"rb\").read())\n",
    "    le = pickle.loads(open('output/le.pickle', \"rb\").read())\n",
    "    \n",
    "    # load the image, resize it to have a width of 600 pixels (while\n",
    "    # maintaining the aspect ratio), and then grab the image dimensions\n",
    "    image = cv2.imread(imagePath)\n",
    "    image = imutils.resize(image, width=600)\n",
    "    (h, w) = image.shape[:2]\n",
    " \n",
    "    # construct a blob from the image\n",
    "    imageBlob = cv2.dnn.blobFromImage(\n",
    "        cv2.resize(image, (300, 300)), 1.0, (300, 300),\n",
    "        (104.0, 177.0, 123.0), swapRB=False, crop=False)\n",
    " \n",
    "    # apply OpenCV's deep learning-based face detector to localize\n",
    "    # faces in the input image\n",
    "    detector.setInput(imageBlob)\n",
    "    detections = detector.forward()\n",
    "    \n",
    "    # loop over the detections\n",
    "    for i in range(0, detections.shape[2]):\n",
    "        # extract the confidence (i.e., probability) associated with the\n",
    "        # prediction\n",
    "        confidence = detections[0, 0, i, 2]\n",
    " \n",
    "        # filter out weak detections\n",
    "        if confidence > 0.5:\n",
    "            # compute the (x, y)-coordinates of the bounding box for the\n",
    "            # face\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    " \n",
    "            # extract the face ROI\n",
    "            face = image[startY:endY, startX:endX]\n",
    "            (fH, fW) = face.shape[:2]\n",
    " \n",
    "            # ensure the face width and height are sufficiently large\n",
    "            if fW < 20 or fH < 20:\n",
    "                continue\n",
    "            # construct a blob for the face ROI, then pass the blob\n",
    "            # through our face embedding model to obtain the 128-d\n",
    "            # quantification of the face\n",
    "            faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255, (96, 96),\n",
    "                (0, 0, 0), swapRB=True, crop=False)\n",
    "            embedder.setInput(faceBlob)\n",
    "            vec = embedder.forward()\n",
    " \n",
    "            # perform classification to recognize the face\n",
    "            preds = recognizer.predict_proba(vec)[0]\n",
    "            j = np.argmax(preds)\n",
    "            proba = preds[j]\n",
    "            name = le.classes_[j]\n",
    "            \n",
    "            # draw the bounding box of the face along with the associated\n",
    "            # probability\n",
    "            text = \"{}: {:.2f}%\".format(name, proba * 100)\n",
    "            y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "            cv2.rectangle(image, (startX, startY), (endX, endY),\n",
    "                (0, 0, 255), 2)\n",
    "            \n",
    "            #TODO: Handle if 2 faces are given.\n",
    "            #Decision boundary\n",
    "            if (name != 'unknown') and (proba *100) < 50:\n",
    "                print(\"Fraud detected\")\n",
    "            else:\n",
    "                print(name)\n",
    "            cv2.putText(image, text, (startX, y),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    " \n",
    "    # show the output image\n",
    "    cv2.imshow(\"Image\", image)\n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recognize('images/trisha_adrian.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading face recognizer...\n",
      "[INFO] starting video stream...\n",
      "[INFO] elasped time: 15.09\n",
      "[INFO] approx. FPS: 9.87\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrollment(Fullname, unique_id, password, images, account_balance):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def homescreen():\n",
    "    from PIL import ImageTk, Image\n",
    "    window = tk.Tk()\n",
    "    window.title(\"WELCOME\")\n",
    "    label = tk.Label(window, text = \"Welcome to GTbank\").pack()\n",
    "    background_image = 'GUI/bg.jpg'\n",
    "    \n",
    "   \n",
    "    \n",
    "   \n",
    "\n",
    "    window.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.jpg written!\n",
      "1.jpg written!\n",
      "2.jpg written!\n",
      "3.jpg written!\n",
      "4.jpg written!\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
